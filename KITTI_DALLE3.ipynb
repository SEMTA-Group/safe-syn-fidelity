{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Kitti\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_number = 4\n",
    "torch.manual_seed(seed_number)\n",
    "np.random.seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f20997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2966c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the KITTI dataset\n",
    "kitti_real_dataset = Kitti(root='data/data_real/',  transform=ToTensor(), download = False)\n",
    "kitti_syn_dataset = Kitti(root='data/data_syn_DALLE3/',  transform=ToTensor(), download = False)\n",
    "\n",
    "# Get the k-th image in the list () and its ground truth labels\n",
    "image_index = 1\n",
    "# image_index = 1 # With train errors\n",
    "\n",
    "# Note that label_real and label_syn are the same\n",
    "image_real, label_real = kitti_real_dataset[image_index]\n",
    "image_syn, label_syn  = kitti_syn_dataset[image_index]\n",
    "\n",
    "# Display the image with the bounding box\n",
    "fig_real, ax = plt.subplots(1)\n",
    "ax.imshow(image_real.permute(1, 2, 0))\n",
    "for box in label_real:\n",
    "    bbox = box[\"bbox\"]\n",
    "    x1 = bbox[0]\n",
    "    y1 = bbox[1]\n",
    "    x2 = bbox[2]\n",
    "    y2 = bbox[3]\n",
    "    #x1, y1, x2, y2 = box\n",
    "\n",
    "    if box[\"type\"] == \"Pedestrian\":\n",
    "        ax.text((x1+x2)/2, y1, 'pedestrian', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='red')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=1)\n",
    "    elif box[\"type\"] == \"Car\":\n",
    "        ax.text((x1+x2)/2, y1, 'car', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='green')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='green', linewidth=1)\n",
    "    elif box[\"type\"] == \"Van\":\n",
    "        ax.text((x1+x2)/2, y1, 'van', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='blue')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='blue', linewidth=1)\n",
    "    elif box[\"type\"] == \"Truck\":\n",
    "        ax.text((x1+x2)/2, y1, 'truck', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='orange')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='orange', linewidth=1)\n",
    "    elif box[\"type\"] == \"Cyclist\":\n",
    "        ax.text((x1+x2)/2, y1, 'cyclist', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='orange')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='orange', linewidth=1)\n",
    "    else:\n",
    "        # For the other classes, we use gray\n",
    "        ax.text((x1+x2)/2, y1, box[\"type\"], ha='center', va='bottom', transform=ax.transData, fontsize=8, color='gray')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='gray', linewidth=1) \n",
    "    ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "fig_syn, ax = plt.subplots(1)\n",
    "ax.imshow(image_syn.permute(1, 2, 0))\n",
    "for box in label_syn:\n",
    "    bbox = box[\"bbox\"]\n",
    "    x1 = bbox[0]\n",
    "    y1 = bbox[1]\n",
    "    x2 = bbox[2]\n",
    "    y2 = bbox[3]\n",
    "    #x1, y1, x2, y2 = box\n",
    "\n",
    "    if (box[\"type\"] == \"Pedestrian\" or box[\"type\"] == \"person\"):\n",
    "        ax.text((x1+x2)/2, y1, 'pedestrian', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='red')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=1)\n",
    "    elif (box[\"type\"].lower() == \"car\"):\n",
    "        ax.text((x1+x2)/2, y1, 'car', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='green')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='green', linewidth=1)\n",
    "    elif box[\"type\"] == \"Van\":\n",
    "        ax.text((x1+x2)/2, y1, 'van', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='blue')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='blue', linewidth=1)\n",
    "    elif box[\"type\"] == \"Truck\":\n",
    "        ax.text((x1+x2)/2, y1, 'truck', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='orange')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='orange', linewidth=1)\n",
    "    elif box[\"type\"] == \"Cyclist\":\n",
    "        ax.text((x1+x2)/2, y1, 'cyclist', ha='center', va='bottom', transform=ax.transData, fontsize=8, color='orange')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='orange', linewidth=1)\n",
    "    else:\n",
    "        # For the other classes, we use gray\n",
    "        ax.text((x1+x2)/2, y1, box[\"type\"], ha='center', va='bottom', transform=ax.transData, fontsize=8, color='gray')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='gray', linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import Kitti\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained model (note that this is not really ideal, as the model has more classes based on COCO)\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Get an image from the KITTI dataset\n",
    "image_real, _ = kitti_real_dataset[image_index]\n",
    "image_real = image_real.to(device)\n",
    "\n",
    "# Generate predictions for the image\n",
    "with torch.no_grad():\n",
    "    prediction = model([image_real])\n",
    "\n",
    "# Display the image with the predicted bounding boxes\n",
    "image_real = image_real.cpu()\n",
    "prediction = prediction[0]\n",
    "boxes = prediction['boxes'].cpu().numpy()\n",
    "scores = prediction['scores'].cpu().numpy()\n",
    "labels = prediction['labels'].cpu().numpy()\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image_real.permute(1, 2, 0))\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    if score > 0.5:\n",
    "        x1, y1, x2, y2 = box\n",
    "        #print(label)\n",
    "        ax.text((x1+x2)/2, y1, COCO_INSTANCE_CATEGORY_NAMES[label], ha='center', va='bottom', transform=ax.transData, fontsize=8, color='red')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        print(COCO_INSTANCE_CATEGORY_NAMES[label])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Get an image from the KITTI dataset\n",
    "image_syn, _ = kitti_syn_dataset[image_index]\n",
    "image_syn = image_syn.to(device)\n",
    "\n",
    "# Generate predictions for the image\n",
    "with torch.no_grad():\n",
    "    prediction = model([image_syn])\n",
    "\n",
    "# Display the image with the predicted bounding boxes\n",
    "image_syn = image_syn.cpu()\n",
    "prediction = prediction[0]\n",
    "boxes = prediction['boxes'].cpu().numpy()\n",
    "scores = prediction['scores'].cpu().numpy()\n",
    "labels = prediction['labels'].cpu().numpy()\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image_syn.permute(1, 2, 0))\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    if score > CONFIDENCE_THRESHOLD:\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "        ax.text((x1+x2)/2, y1, COCO_INSTANCE_CATEGORY_NAMES[label], ha='center', va='bottom', transform=ax.transData, fontsize=8, color='red')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        print(COCO_INSTANCE_CATEGORY_NAMES[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1addcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    def __init__(self, x1, y1, x2, y2, cat = \"dc\"):\n",
    "        self.x1 = x1\n",
    "        self.y1 = y1\n",
    "        self.x2 = x2\n",
    "        self.y2 = y2\n",
    "        self.cat = cat\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    x_left = max(box1.x1, box2.x1)\n",
    "    y_top = max(box1.y1, box2.y1)\n",
    "    x_right = min(box1.x2, box2.x2)\n",
    "    y_bottom = min(box1.y2, box2.y2)\n",
    "\n",
    "    # Calculate area of intersection\n",
    "    intersection_area = max(0, x_right - x_left) * max(0, y_bottom - y_top)\n",
    "\n",
    "    # Calculate areas of the input boxes\n",
    "    box1_area = (box1.x2 - box1.x1) * (box1.y2 - box1.y1)\n",
    "    box2_area = (box2.x2 - box2.x1) * (box2.y2 - box2.y1)\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "def count_undetected_objects(ground_truths, predictions, iou_threshold, size_gt = 0):\n",
    "    undetected_count = 0\n",
    "    count_smaller = 0\n",
    "    undetected_count_smaller = 0\n",
    "\n",
    "    for gt_box in ground_truths:\n",
    "        area = (gt_box.x2 - gt_box.x1) * (gt_box.y2 - gt_box.y1)        \n",
    "        if area >= size_gt[\"dc\"]:\n",
    "            detected = False\n",
    "            for pred_box in predictions:\n",
    "                if calculate_iou(gt_box, pred_box) >= iou_threshold:\n",
    "                    detected = True\n",
    "                    break\n",
    "            if not detected:\n",
    "                undetected_count += 1\n",
    "        else:\n",
    "            count_smaller += 1\n",
    "            detected = False\n",
    "            for pred_box in predictions:\n",
    "                if calculate_iou(gt_box, pred_box) >= iou_threshold:\n",
    "                    detected = True\n",
    "                    break\n",
    "            if not detected:\n",
    "                undetected_count_smaller += 1            \n",
    "\n",
    "\n",
    "    return undetected_count, count_smaller, undetected_count_smaller\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fea8bc",
   "metadata": {},
   "source": [
    "### Work on the analysis of the car/train/van/truck class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e990a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of strings\n",
    "real_set = {\"car\", \"van\", \"truck\", \"train\"}\n",
    "syn_set = {\"car\", \"train\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AREA_THRESHOLD = {'car': 10000, 'pedestrian': 100, 'dc': 1000}\n",
    "\n",
    "bounding_boxes_real = list()\n",
    "bounding_boxes_real_label = list()\n",
    "bounding_boxes_syn = list()\n",
    "bounding_boxes_syn_label = list()\n",
    "\n",
    "# Get an image from the KITTI dataset\n",
    "image_real, label_real = kitti_real_dataset[image_index]\n",
    "image_real = image_real.to(device)\n",
    "\n",
    "# Generate predictions for the image\n",
    "with torch.no_grad():\n",
    "    prediction_real = model([image_real])\n",
    "\n",
    "# Display the image with the predicted bounding boxes\n",
    "image_real = image_real.cpu()\n",
    "prediction_real = prediction_real[0]\n",
    "boxes_real = prediction_real['boxes'].cpu().numpy()\n",
    "scores_real = prediction_real['scores'].cpu().numpy()\n",
    "classes_real = prediction_real['labels'].cpu().numpy()\n",
    "\n",
    "for box_real, score_real, class_real in zip(boxes_real, scores_real, classes_real):\n",
    "    if score_real > CONFIDENCE_THRESHOLD and COCO_INSTANCE_CATEGORY_NAMES[class_real].lower() in real_set:\n",
    "        x1, y1, x2, y2 = box_real\n",
    "        bounding_boxes_real.append(BoundingBox(x1, y1, x2, y2))\n",
    "\n",
    "\n",
    "for box in label_real:\n",
    "    bbox = box[\"bbox\"]\n",
    "    x1 = bbox[0]\n",
    "    y1 = bbox[1]\n",
    "    x2 = bbox[2]\n",
    "    y2 = bbox[3]        \n",
    "    if (box[\"type\"].lower() in real_set):\n",
    "        bounding_boxes_real_label.append(BoundingBox(x1, y1, x2, y2))\n",
    "\n",
    "# Get an image from the KITTI dataset\n",
    "image_syn, label_syn = kitti_syn_dataset[image_index]\n",
    "image_syn = image_syn.to(device)\n",
    "\n",
    "# Generate predictions for the image\n",
    "with torch.no_grad():\n",
    "    prediction = model([image_syn])\n",
    "\n",
    "    \n",
    "    \n",
    "# Display the image with the predicted bounding boxes\n",
    "image_syn = image_syn.cpu()\n",
    "prediction_syn = prediction[0]\n",
    "boxes_syn = prediction_syn['boxes'].cpu().numpy()\n",
    "scores_syn = prediction_syn['scores'].cpu().numpy()\n",
    "classes_syn = prediction_syn['labels'].cpu().numpy()\n",
    "\n",
    "for box_syn, score_syn, class_syn in zip(boxes_syn, scores_syn, classes_syn):\n",
    "    if score_syn > CONFIDENCE_THRESHOLD and COCO_INSTANCE_CATEGORY_NAMES[class_syn].lower() in syn_set:\n",
    "        x1, y1, x2, y2 = box_syn\n",
    "        bounding_boxes_syn.append(BoundingBox(x1, y1, x2, y2))\n",
    "\n",
    "for box in label_syn:\n",
    "    bbox = box[\"bbox\"]\n",
    "    x1 = bbox[0]\n",
    "    y1 = bbox[1]\n",
    "    x2 = bbox[2]\n",
    "    y2 = bbox[3]        \n",
    "    if (box[\"type\"].lower() in syn_set):\n",
    "        bounding_boxes_syn_label.append(BoundingBox(x1, y1, x2, y2))\n",
    "              \n",
    "        \n",
    "        \n",
    "undetected_count_real, count_smaller_real, undetected_count_smaller = count_undetected_objects(bounding_boxes_real_label, bounding_boxes_real, IOU_THRESHOLD, AREA_THRESHOLD)\n",
    "counter_larger_real = len(bounding_boxes_real_label) - count_smaller_real\n",
    "\n",
    "print(\"=== Summary comparing the detector in real ===\")\n",
    "print(\"Undetected in real images by DNN (area larger than threshold):\", undetected_count_real)\n",
    "# print(\"Ground-truth smaller than threshold:\", count_smaller_real)\n",
    "\n",
    "ghost_count, _ , _ = count_undetected_objects(bounding_boxes_real, bounding_boxes_real_label, IOU_THRESHOLD, AREA_THRESHOLD)\n",
    "print(\"Ghost objects in real images by DNN (area larger than threshold):\", ghost_count)\n",
    "\n",
    "undetected_count_syn, count_smaller_syn, undetected_count_smaller = count_undetected_objects(bounding_boxes_syn_label, bounding_boxes_syn, IOU_THRESHOLD, AREA_THRESHOLD)\n",
    "counter_larger_syn = len(bounding_boxes_syn_label) - count_smaller_syn\n",
    "\n",
    "print(\"=== Summary comparing the detector in syn ===\")\n",
    "print(\"Undetected in syn images by DNN (area larger than threshold):\", undetected_count_syn)\n",
    "# print(\"Ground-truth smaller than threshold:\", count_smaller_syn)\n",
    "\n",
    "ghost_count, _ , _ = count_undetected_objects(bounding_boxes_syn, bounding_boxes_syn_label, IOU_THRESHOLD, AREA_THRESHOLD)\n",
    "print(\"Ghost objects in syn images by DNN (area larger than threshold):\", ghost_count)\n",
    "\n",
    "print(\"=== Performance evaluation in single image pair, based on the discrepency ratio difference on FN ===\")\n",
    "print(\"Total of objects larger than the size threshold while in the class of interest, in real: \" +str( counter_larger_real))\n",
    "print(\"Total of objects larger than the size threshold while in the class of interest, in syn: \" +str( counter_larger_syn))\n",
    "\n",
    "fn_rate_real = undetected_count_real / counter_larger_real if counter_larger_real > 0 else 0\n",
    "fn_rate_syn = undetected_count_syn / counter_larger_syn if counter_larger_syn > 0 else 0\n",
    "print(\"discrepency in FN = \"+str(abs(fn_rate_real - fn_rate_syn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a5f7d",
   "metadata": {},
   "source": [
    "### Image processing by controlling  certain parameters\n",
    "\n",
    "In image processing with libraries like Pillow, the parameters used for adjusting attributes like brightness, contrast, saturation, etc., often have specific ranges, usually centered around a default value that represents the unaltered state of the image. Here's a general idea of how these ranges typically work:\n",
    "\n",
    "* Brightness: The factor usually ranges from 0 to any positive number. A factor of 1 means no change, less than 1 makes the image darker, and greater than 1 makes the image brighter. There is typically no strict upper limit, but practical values are usually close to 1 (e.g., 0.5 for darker, 1.5 for brighter).\n",
    "\n",
    "* Contrast: Similar to brightness, a factor of 1 indicates no change. Values greater than 1 increase the contrast, and values less than 1 decrease it. The scale is generally the same as brightness.\n",
    "\n",
    "* Saturation: Again, a value of 1 means no change. Values less than 1 desaturate the image, potentially down to 0 which would be completely grayscale. Values greater than 1 increase the saturation.\n",
    "\n",
    "* Sharpness: A value of 1 represents the original image sharpness. Increasing the value sharpens the image, while decreasing it (down to 0) will blur the image.\n",
    "\n",
    "* Gamma: Gamma correction doesn't have a fixed range, but it's typically a positive number. A value of 1 implies no change, while values less than 1 darken the image and values greater than 1 lighten it. The effect is nonlinear and affects mid-tones while preserving blacks and whites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de36a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image from the KITTI dataset\n",
    "image_syn, _ = kitti_syn_dataset[image_index]\n",
    "image_syn = image_syn.to(device)\n",
    "\n",
    "# Step 1: Convert PyTorch tensor to PIL Image\n",
    "# Ensure tensor is in CPU and convert to PIL\n",
    "img_pil = transforms.ToPILImage()(image_syn.cpu())\n",
    "\n",
    "# Step 2: Enhance the image using Pillow\n",
    "contrast_enhancer = ImageEnhance.Contrast(img_pil)\n",
    "contrast_factor = 1\n",
    "img_pil_enhanced = contrast_enhancer.enhance(contrast_factor)  # Example factor, adjust as needed\n",
    "\n",
    "# Enhance the brightness\n",
    "brightness_enhancer = ImageEnhance.Brightness(img_pil_enhanced)\n",
    "brightness_factor = 1\n",
    "img_pil_enhanced = brightness_enhancer.enhance(brightness_factor)  # Example factor, adjust as needed\n",
    "\n",
    "# Enhance the saturation\n",
    "saturation_enhancer = ImageEnhance.Color(img_pil_enhanced)\n",
    "saturation_factor = 1\n",
    "img_pil_enhanced = saturation_enhancer.enhance(saturation_factor)  # Example factor, adjust as needed\n",
    "\n",
    "# Enhance the sharpness\n",
    "sharpness_enhancer = ImageEnhance.Sharpness(img_pil_enhanced)\n",
    "sharpness_factor = 1\n",
    "img_pil_enhanced = sharpness_enhancer.enhance(sharpness_factor)  # Example factor, adjust as needed\n",
    "\n",
    "\n",
    "# Step 3: Convert the enhanced PIL Image back to PyTorch tensor\n",
    "transform = transforms.ToTensor()\n",
    "image_syn_enhanced = transform(img_pil_enhanced)\n",
    "image_syn_enhanced = image_syn_enhanced.to(device)\n",
    "\n",
    "# Generate predictions for the image\n",
    "with torch.no_grad():\n",
    "    prediction = model([image_syn_enhanced])\n",
    "\n",
    "# Display the image with the predicted bounding boxes\n",
    "image_syn_enhanced = image_syn_enhanced.cpu()\n",
    "prediction = prediction[0]\n",
    "boxes = prediction['boxes'].cpu().numpy()\n",
    "scores = prediction['scores'].cpu().numpy()\n",
    "labels = prediction['labels'].cpu().numpy()\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image_syn_enhanced.permute(1, 2, 0))\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    if score > CONFIDENCE_THRESHOLD:\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "        ax.text((x1+x2)/2, y1, COCO_INSTANCE_CATEGORY_NAMES[label], ha='center', va='bottom', transform=ax.transData, fontsize=8, color='red')\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        print(COCO_INSTANCE_CATEGORY_NAMES[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec26eb3",
   "metadata": {},
   "source": [
    "### Experiment \n",
    "\n",
    "Step 1, try the value with an increased interval of 0.1, from 0.8 to 1.2, making us reaching 125 tests.\n",
    "Step 2, based on the result, perform local refinement with an increased interval of 0.05 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating a list from 0.7 to 1.3 with an increment of 0.1 using numpy's arange function\n",
    "\n",
    "# (1.1, 1.0, 0.8)\n",
    "\n",
    "contrast_start = 0.7\n",
    "contrast_end = 1.3\n",
    "\n",
    "brightness_start = 0.7\n",
    "brightness_end = 1.3\n",
    "\n",
    "sharpness_start = 0.7\n",
    "sharpness_end = 1.3\n",
    "\n",
    "step = 0.1\n",
    "\n",
    "contrast_elements_list = [round(i, 2) for i in np.arange(contrast_start, contrast_end + step, step)]\n",
    "print(contrast_elements_list)\n",
    "\n",
    "brightness_elements_list = [round(i, 2) for i in np.arange(brightness_start, brightness_end + step, step)]\n",
    "print(brightness_elements_list)\n",
    "\n",
    "sharpness_elements_list = [round(i, 2) for i in np.arange(sharpness_start, sharpness_end + step, step)]\n",
    "print(sharpness_elements_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AREA_THRESHOLD = {'car': 10000, 'pedestrian': 100, 'dc': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of strings that we only wish to perform analysis. \n",
    "real_set = {\"car\", \"van\", \"truck\", \"train\"}\n",
    "syn_set = {\"car\", \"train\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ad8e7",
   "metadata": {},
   "source": [
    "\n",
    "#### Set the range of the dataset to be iterated. \n",
    "Currently, we only have 20+ synthetic images, so we set the range to be 20. Change the number when the number of synthetic images has increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b27587",
   "metadata": {},
   "source": [
    "Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_inconsistency_ratio = 10000000\n",
    "contrast, brightness, sharpness = -1, -1, -1\n",
    "\n",
    "for contrast_factor in contrast_elements_list:\n",
    "    for brightness_factor in brightness_elements_list:\n",
    "        for sharpness_factor in sharpness_elements_list:\n",
    "            \n",
    "            inconsistency_ratio = 0\n",
    "            \n",
    "            for image_index in range(len(kitti_syn_dataset)):\n",
    "                \n",
    "\n",
    "                bounding_boxes_real = list()\n",
    "                bounding_boxes_real_label = list()\n",
    "                bounding_boxes_syn = list()\n",
    "                bounding_boxes_syn_label = list()\n",
    "\n",
    "                # Get an image from the KITTI dataset\n",
    "                image_real, label_real = kitti_real_dataset[image_index]\n",
    "                image_real = image_real.to(device)\n",
    "\n",
    "                # Generate predictions for the image\n",
    "                with torch.no_grad():\n",
    "                    prediction_real = model([image_real])\n",
    "\n",
    "                # Display the image with the predicted bounding boxes\n",
    "                image_real = image_real.cpu()\n",
    "                prediction_real = prediction_real[0]\n",
    "                boxes_real = prediction_real['boxes'].cpu().numpy()\n",
    "                scores_real = prediction_real['scores'].cpu().numpy()\n",
    "                classes_real = prediction_real['labels'].cpu().numpy()\n",
    "\n",
    "                for box_real, score_real, class_real in zip(boxes_real, scores_real, classes_real):\n",
    "                    if score_real > CONFIDENCE_THRESHOLD and COCO_INSTANCE_CATEGORY_NAMES[class_real].lower() in real_set:\n",
    "                        x1, y1, x2, y2 = box_real\n",
    "                        bounding_boxes_real.append(BoundingBox(x1, y1, x2, y2))\n",
    "\n",
    "\n",
    "                for box in label_real:\n",
    "                    bbox = box[\"bbox\"]\n",
    "                    x1 = bbox[0]\n",
    "                    y1 = bbox[1]\n",
    "                    x2 = bbox[2]\n",
    "                    y2 = bbox[3]        \n",
    "                    if (box[\"type\"].lower() in real_set):\n",
    "                        bounding_boxes_real_label.append(BoundingBox(x1, y1, x2, y2))\n",
    "\n",
    "                \n",
    "                # Get an image from the KITTI dataset\n",
    "                image_syn, label_syn = kitti_syn_dataset[image_index]\n",
    "\n",
    "                # Step 1: Convert PyTorch tensor to PIL Image\n",
    "                # Ensure tensor is in CPU and convert to PIL\n",
    "                img_pil = transforms.ToPILImage()(image_syn)\n",
    "\n",
    "                # Step 2: Enhance the image using Pillow\n",
    "                contrast_enhancer = ImageEnhance.Contrast(img_pil)\n",
    "                img_pil_enhanced = contrast_enhancer.enhance(contrast_factor)  \n",
    "\n",
    "                # Enhance the brightness\n",
    "                brightness_enhancer = ImageEnhance.Brightness(img_pil_enhanced)\n",
    "                img_pil_enhanced = brightness_enhancer.enhance(brightness_factor)  \n",
    "\n",
    "                # Enhance the sharpness\n",
    "                sharpness_enhancer = ImageEnhance.Sharpness(img_pil_enhanced)\n",
    "                img_pil_enhanced = sharpness_enhancer.enhance(sharpness_factor) \n",
    "\n",
    "\n",
    "                # Step 3: Convert the enhanced PIL Image back to PyTorch tensor\n",
    "                transform = transforms.ToTensor()\n",
    "                image_syn_enhanced = transform(img_pil_enhanced)\n",
    "                image_syn_enhanced = image_syn_enhanced.to(device)\n",
    "\n",
    "                # Generate predictions for the image\n",
    "                with torch.no_grad():\n",
    "                    prediction = model([image_syn_enhanced])\n",
    "                \n",
    "\n",
    "                image_syn = image_syn_enhanced.cpu()\n",
    "                prediction_syn = prediction[0]\n",
    "                boxes_syn = prediction_syn['boxes'].cpu().numpy()\n",
    "                scores_syn = prediction_syn['scores'].cpu().numpy()\n",
    "                classes_syn = prediction_syn['labels'].cpu().numpy()\n",
    "\n",
    "                for box_syn, score_syn, class_syn in zip(boxes_syn, scores_syn, classes_syn):\n",
    "                    if score_syn > CONFIDENCE_THRESHOLD and COCO_INSTANCE_CATEGORY_NAMES[class_syn].lower() in syn_set:\n",
    "                        x1, y1, x2, y2 = box_syn\n",
    "                        bounding_boxes_syn.append(BoundingBox(x1, y1, x2, y2))\n",
    "\n",
    "                for box in label_syn:\n",
    "                    bbox = box[\"bbox\"]\n",
    "                    x1 = bbox[0]\n",
    "                    y1 = bbox[1]\n",
    "                    x2 = bbox[2]\n",
    "                    y2 = bbox[3]        \n",
    "                    if (box[\"type\"].lower() in syn_set):\n",
    "                        bounding_boxes_syn_label.append(BoundingBox(x1, y1, x2, y2))\n",
    "                \n",
    "                undetected_count_real, count_smaller_real, undetected_count_smaller = count_undetected_objects(bounding_boxes_real_label, bounding_boxes_real, IOU_THRESHOLD, AREA_THRESHOLD)\n",
    "                counter_larger_real = len(bounding_boxes_real_label) - count_smaller_real\n",
    "                undetected_count_syn, count_smaller_syn, undetected_count_smaller = count_undetected_objects(bounding_boxes_syn_label, bounding_boxes_syn, IOU_THRESHOLD, AREA_THRESHOLD)\n",
    "                counter_larger_syn = len(bounding_boxes_syn_label) - count_smaller_syn\n",
    "            \n",
    "                fn_rate_real = undetected_count_real / counter_larger_real if counter_larger_real > 0 else 0\n",
    "                fn_rate_syn = undetected_count_syn / counter_larger_syn if counter_larger_syn > 0 else 0\n",
    "                inconsistency_ratio = inconsistency_ratio + abs(fn_rate_real - fn_rate_syn)\n",
    "            \n",
    "            print(contrast_factor, brightness_factor, sharpness_factor, inconsistency_ratio, sep=', ')\n",
    "            \n",
    "            if (inconsistency_ratio < min_inconsistency_ratio):\n",
    "                min_inconsistency_ratio = inconsistency_ratio\n",
    "                contrast, brightness, sharpness = contrast_factor, brightness_factor, sharpness_factor\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The safe-output fidelity can be reduced by setting (contrast, brightness, sharpness) to be (\"+str(contrast)+\n",
    "      \", \"+str(brightness)+\", \"+str(sharpness)+\") in Pillow\")\n",
    "\n",
    "print(\"The minimum of safety-critical inconsistent KPI (difference in FN ratio) are \"+str(min_inconsistency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95ac5c",
   "metadata": {},
   "source": [
    "Results in optimization listed below, where the minimum occurs in the following configuration\n",
    "\n",
    "* 0.8, 0.8, 1.4, 4.699007936507936\n",
    "* 0.8, 0.9, 1.4, 4.699007936507936\n",
    "\n",
    "\n",
    "When nothing occurs, \n",
    "* 1.0, 1.0, 1.0, 5.420039682539683\n",
    "\n",
    "\n",
    "The worst case occurs,\n",
    "0.7, 1.3, 1.3, 7.183531746031746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7, 0.7, 0.7, 5.462896825396825\n",
    "0.7, 0.7, 0.8, 5.320039682539683\n",
    "0.7, 0.7, 0.9, 5.320039682539683\n",
    "0.7, 0.7, 1.0, 5.2307539682539685\n",
    "0.7, 0.7, 1.1, 5.341865079365079\n",
    "0.7, 0.7, 1.2, 5.341865079365079\n",
    "0.7, 0.7, 1.3, 5.341865079365079\n",
    "0.7, 0.7, 1.4, 4.841865079365078\n",
    "0.7, 0.8, 0.7, 5.462896825396825\n",
    "0.7, 0.8, 0.8, 5.320039682539683\n",
    "0.7, 0.8, 0.9, 5.320039682539683\n",
    "0.7, 0.8, 1.0, 5.2307539682539685\n",
    "0.7, 0.8, 1.1, 5.341865079365079\n",
    "0.7, 0.8, 1.2, 4.841865079365078\n",
    "0.7, 0.8, 1.3, 4.841865079365078\n",
    "0.7, 0.8, 1.4, 4.841865079365078\n",
    "0.7, 0.9, 0.7, 5.462896825396825\n",
    "0.7, 0.9, 0.8, 5.320039682539683\n",
    "0.7, 0.9, 0.9, 5.320039682539683\n",
    "0.7, 0.9, 1.0, 5.320039682539683\n",
    "0.7, 0.9, 1.1, 4.841865079365078\n",
    "0.7, 0.9, 1.2, 4.841865079365078\n",
    "0.7, 0.9, 1.3, 4.841865079365078\n",
    "0.7, 0.9, 1.4, 4.841865079365078\n",
    "0.7, 1.0, 0.7, 5.574007936507937\n",
    "0.7, 1.0, 0.8, 5.462896825396825\n",
    "0.7, 1.0, 0.9, 5.462896825396825\n",
    "0.7, 1.0, 1.0, 5.574007936507937\n",
    "0.7, 1.0, 1.1, 5.574007936507937\n",
    "0.7, 1.0, 1.2, 5.574007936507937\n",
    "0.7, 1.0, 1.3, 4.984722222222222\n",
    "0.7, 1.0, 1.4, 5.9847222222222225\n",
    "0.7, 1.1, 0.7, 5.574007936507937\n",
    "0.7, 1.1, 0.8, 5.574007936507937\n",
    "0.7, 1.1, 0.9, 5.574007936507937\n",
    "0.7, 1.1, 1.0, 5.574007936507937\n",
    "0.7, 1.1, 1.1, 5.574007936507937\n",
    "0.7, 1.1, 1.2, 5.574007936507937\n",
    "0.7, 1.1, 1.3, 6.574007936507936\n",
    "0.7, 1.1, 1.4, 6.040674603174603\n",
    "0.7, 1.2, 0.7, 5.574007936507937\n",
    "0.7, 1.2, 0.8, 5.574007936507937\n",
    "0.7, 1.2, 0.9, 5.574007936507937\n",
    "0.7, 1.2, 1.0, 5.574007936507937\n",
    "0.7, 1.2, 1.1, 5.540674603174603\n",
    "0.7, 1.2, 1.2, 5.540674603174603\n",
    "0.7, 1.2, 1.3, 5.540674603174603\n",
    "0.7, 1.2, 1.4, 5.040674603174603\n",
    "0.7, 1.3, 0.7, 5.2728174603174605\n",
    "0.7, 1.3, 0.8, 5.415674603174603\n",
    "0.7, 1.3, 0.9, 5.540674603174603\n",
    "0.7, 1.3, 1.0, 5.874007936507936\n",
    "0.7, 1.3, 1.1, 5.874007936507936\n",
    "0.7, 1.3, 1.2, 6.040674603174603\n",
    "0.7, 1.3, 1.3, 7.183531746031746\n",
    "0.7, 1.3, 1.4, 6.594246031746032\n",
    "0.7, 1.4, 0.7, 6.637896825396825\n",
    "0.7, 1.4, 0.8, 6.749007936507936\n",
    "0.7, 1.4, 0.9, 6.749007936507936\n",
    "0.7, 1.4, 1.0, 6.874007936507936\n",
    "0.7, 1.4, 1.1, 6.874007936507936\n",
    "0.7, 1.4, 1.2, 7.183531746031746\n",
    "0.7, 1.4, 1.3, 6.683531746031746\n",
    "0.7, 1.4, 1.4, 6.580357142857143\n",
    "0.8, 0.7, 0.7, 5.17718253968254\n",
    "0.8, 0.7, 0.8, 5.17718253968254\n",
    "0.8, 0.7, 0.9, 5.17718253968254\n",
    "0.8, 0.7, 1.0, 5.087896825396825\n",
    "0.8, 0.7, 1.1, 5.087896825396825\n",
    "0.8, 0.7, 1.2, 5.199007936507937\n",
    "0.8, 0.7, 1.3, 5.087896825396825\n",
    "0.8, 0.7, 1.4, 5.199007936507937\n",
    "0.8, 0.8, 0.7, 5.320039682539683\n",
    "0.8, 0.8, 0.8, 5.17718253968254\n",
    "0.8, 0.8, 0.9, 5.17718253968254\n",
    "0.8, 0.8, 1.0, 5.17718253968254\n",
    "0.8, 0.8, 1.1, 5.087896825396825\n",
    "0.8, 0.8, 1.2, 5.087896825396825\n",
    "0.8, 0.8, 1.3, 5.087896825396825\n",
    "0.8, 0.8, 1.4, 4.699007936507936\n",
    "0.8, 0.9, 0.7, 5.320039682539683\n",
    "0.8, 0.9, 0.8, 5.320039682539683\n",
    "0.8, 0.9, 0.9, 5.320039682539683\n",
    "0.8, 0.9, 1.0, 5.320039682539683\n",
    "0.8, 0.9, 1.1, 5.431150793650794\n",
    "0.8, 0.9, 1.2, 5.199007936507937\n",
    "0.8, 0.9, 1.3, 5.199007936507937\n",
    "0.8, 0.9, 1.4, 4.699007936507936\n",
    "0.8, 1.0, 0.7, 5.462896825396825\n",
    "0.8, 1.0, 0.8, 5.462896825396825\n",
    "0.8, 1.0, 0.9, 5.462896825396825\n",
    "0.8, 1.0, 1.0, 5.462896825396825\n",
    "0.8, 1.0, 1.1, 5.574007936507937\n",
    "0.8, 1.0, 1.2, 5.574007936507937\n",
    "0.8, 1.0, 1.3, 5.574007936507937\n",
    "0.8, 1.0, 1.4, 5.574007936507937\n",
    "0.8, 1.1, 0.7, 5.462896825396825\n",
    "0.8, 1.1, 0.8, 5.462896825396825\n",
    "0.8, 1.1, 0.9, 5.462896825396825\n",
    "0.8, 1.1, 1.0, 5.574007936507937\n",
    "0.8, 1.1, 1.1, 5.574007936507937\n",
    "0.8, 1.1, 1.2, 5.574007936507937\n",
    "0.8, 1.1, 1.3, 5.574007936507937\n",
    "0.8, 1.1, 1.4, 5.540674603174603\n",
    "0.8, 1.2, 0.7, 5.531150793650793\n",
    "0.8, 1.2, 0.8, 5.6740079365079366\n",
    "0.8, 1.2, 0.9, 5.6740079365079366\n",
    "0.8, 1.2, 1.0, 5.6740079365079366\n",
    "0.8, 1.2, 1.1, 5.9740079365079355\n",
    "0.8, 1.2, 1.2, 5.9740079365079355\n",
    "0.8, 1.2, 1.3, 5.9740079365079355\n",
    "0.8, 1.2, 1.4, 5.874007936507936\n",
    "0.8, 1.3, 0.7, 6.620039682539682\n",
    "0.8, 1.3, 0.8, 6.620039682539682\n",
    "0.8, 1.3, 0.9, 6.862896825396825\n",
    "0.8, 1.3, 1.0, 6.862896825396825\n",
    "0.8, 1.3, 1.1, 6.9740079365079355\n",
    "0.8, 1.3, 1.2, 6.9740079365079355\n",
    "0.8, 1.3, 1.3, 6.9740079365079355\n",
    "0.8, 1.3, 1.4, 7.116865079365079\n",
    "0.8, 1.4, 0.7, 6.762896825396825\n",
    "0.8, 1.4, 0.8, 6.762896825396825\n",
    "0.8, 1.4, 0.9, 6.874007936507936\n",
    "0.8, 1.4, 1.0, 6.874007936507936\n",
    "0.8, 1.4, 1.1, 6.9740079365079355\n",
    "0.8, 1.4, 1.2, 6.9740079365079355\n",
    "0.8, 1.4, 1.3, 6.9740079365079355\n",
    "0.8, 1.4, 1.4, 6.4740079365079355\n",
    "0.9, 0.7, 0.7, 5.17718253968254\n",
    "0.9, 0.7, 0.8, 5.17718253968254\n",
    "0.9, 0.7, 0.9, 5.034325396825397\n",
    "0.9, 0.7, 1.0, 5.17718253968254\n",
    "0.9, 0.7, 1.1, 5.087896825396825\n",
    "0.9, 0.7, 1.2, 5.087896825396825\n",
    "0.9, 0.7, 1.3, 5.087896825396825\n",
    "0.9, 0.7, 1.4, 5.087896825396825\n",
    "0.9, 0.8, 0.7, 5.17718253968254\n",
    "0.9, 0.8, 0.8, 5.17718253968254\n",
    "0.9, 0.8, 0.9, 5.17718253968254\n",
    "0.9, 0.8, 1.0, 5.17718253968254\n",
    "0.9, 0.8, 1.1, 5.17718253968254\n",
    "0.9, 0.8, 1.2, 5.087896825396825\n",
    "0.9, 0.8, 1.3, 5.087896825396825\n",
    "0.9, 0.8, 1.4, 5.087896825396825\n",
    "0.9, 0.9, 0.7, 5.17718253968254\n",
    "0.9, 0.9, 0.8, 5.320039682539683\n",
    "0.9, 0.9, 0.9, 5.320039682539683\n",
    "0.9, 0.9, 1.0, 5.320039682539683\n",
    "0.9, 0.9, 1.1, 5.320039682539683\n",
    "0.9, 0.9, 1.2, 5.320039682539683\n",
    "0.9, 0.9, 1.3, 5.320039682539683\n",
    "0.9, 0.9, 1.4, 5.431150793650794\n",
    "0.9, 1.0, 0.7, 5.320039682539683\n",
    "0.9, 1.0, 0.8, 5.320039682539683\n",
    "0.9, 1.0, 0.9, 5.420039682539683\n",
    "0.9, 1.0, 1.0, 5.420039682539683\n",
    "0.9, 1.0, 1.1, 5.531150793650793\n",
    "0.9, 1.0, 1.2, 5.531150793650793\n",
    "0.9, 1.0, 1.3, 5.531150793650793\n",
    "0.9, 1.0, 1.4, 5.531150793650793\n",
    "0.9, 1.1, 0.7, 5.420039682539683\n",
    "0.9, 1.1, 0.8, 5.420039682539683\n",
    "0.9, 1.1, 0.9, 5.562896825396825\n",
    "0.9, 1.1, 1.0, 5.896230158730158\n",
    "0.9, 1.1, 1.1, 6.00734126984127\n",
    "0.9, 1.1, 1.2, 6.00734126984127\n",
    "0.9, 1.1, 1.3, 5.9740079365079355\n",
    "0.9, 1.1, 1.4, 5.9740079365079355\n",
    "0.9, 1.2, 0.7, 5.753373015873016\n",
    "0.9, 1.2, 0.8, 5.753373015873016\n",
    "0.9, 1.2, 0.9, 5.753373015873016\n",
    "0.9, 1.2, 1.0, 5.862896825396825\n",
    "0.9, 1.2, 1.1, 5.9740079365079355\n",
    "0.9, 1.2, 1.2, 5.9740079365079355\n",
    "0.9, 1.2, 1.3, 5.9740079365079355\n",
    "0.9, 1.2, 1.4, 5.9740079365079355\n",
    "0.9, 1.3, 0.7, 5.4771825396825395\n",
    "0.9, 1.3, 0.8, 5.577182539682539\n",
    "0.9, 1.3, 0.9, 5.7200396825396815\n",
    "0.9, 1.3, 1.0, 6.7200396825396815\n",
    "0.9, 1.3, 1.1, 6.831150793650792\n",
    "0.9, 1.3, 1.2, 6.831150793650792\n",
    "0.9, 1.3, 1.3, 6.831150793650792\n",
    "0.9, 1.3, 1.4, 6.831150793650792\n",
    "0.9, 1.4, 0.7, 5.4771825396825395\n",
    "0.9, 1.4, 0.8, 5.620039682539682\n",
    "0.9, 1.4, 0.9, 5.620039682539682\n",
    "0.9, 1.4, 1.0, 5.7311507936507935\n",
    "0.9, 1.4, 1.1, 5.7311507936507935\n",
    "0.9, 1.4, 1.2, 5.7311507936507935\n",
    "0.9, 1.4, 1.3, 5.831150793650793\n",
    "0.9, 1.4, 1.4, 6.331150793650792\n",
    "1.0, 0.7, 0.7, 5.17718253968254\n",
    "1.0, 0.7, 0.8, 5.17718253968254\n",
    "1.0, 0.7, 0.9, 5.034325396825397\n",
    "1.0, 0.7, 1.0, 5.034325396825397\n",
    "1.0, 0.7, 1.1, 5.034325396825397\n",
    "1.0, 0.7, 1.2, 4.945039682539682\n",
    "1.0, 0.7, 1.3, 4.945039682539682\n",
    "1.0, 0.7, 1.4, 4.945039682539682\n",
    "1.0, 0.8, 0.7, 5.17718253968254\n",
    "1.0, 0.8, 0.8, 5.17718253968254\n",
    "1.0, 0.8, 0.9, 5.17718253968254\n",
    "1.0, 0.8, 1.0, 5.17718253968254\n",
    "1.0, 0.8, 1.1, 5.034325396825397\n",
    "1.0, 0.8, 1.2, 5.134325396825397\n",
    "1.0, 0.8, 1.3, 5.17718253968254\n",
    "1.0, 0.8, 1.4, 5.187896825396825\n",
    "1.0, 0.9, 0.7, 5.134325396825397\n",
    "1.0, 0.9, 0.8, 5.134325396825397\n",
    "1.0, 0.9, 0.9, 5.420039682539683\n",
    "1.0, 0.9, 1.0, 5.420039682539683\n",
    "1.0, 0.9, 1.1, 5.420039682539683\n",
    "1.0, 0.9, 1.2, 5.420039682539683\n",
    "1.0, 0.9, 1.3, 5.420039682539683\n",
    "1.0, 0.9, 1.4, 5.420039682539683\n",
    "1.0, 1.0, 0.7, 5.134325396825397\n",
    "1.0, 1.0, 0.8, 5.277182539682539\n",
    "1.0, 1.0, 0.9, 5.277182539682539\n",
    "1.0, 1.0, 1.0, 5.420039682539683\n",
    "1.0, 1.0, 1.1, 5.420039682539683\n",
    "1.0, 1.0, 1.2, 5.420039682539683\n",
    "1.0, 1.0, 1.3, 5.420039682539683\n",
    "1.0, 1.0, 1.4, 5.864484126984126\n",
    "1.0, 1.1, 0.7, 5.277182539682539\n",
    "1.0, 1.1, 0.8, 5.610515873015872\n",
    "1.0, 1.1, 0.9, 5.610515873015872\n",
    "1.0, 1.1, 1.0, 5.610515873015872\n",
    "1.0, 1.1, 1.1, 5.753373015873016\n",
    "1.0, 1.1, 1.2, 5.896230158730159\n",
    "1.0, 1.1, 1.3, 5.862896825396825\n",
    "1.0, 1.1, 1.4, 5.974007936507936\n",
    "1.0, 1.2, 0.7, 5.46765873015873\n",
    "1.0, 1.2, 0.8, 5.46765873015873\n",
    "1.0, 1.2, 0.9, 5.46765873015873\n",
    "1.0, 1.2, 1.0, 5.577182539682539\n",
    "1.0, 1.2, 1.1, 5.608928571428571\n",
    "1.0, 1.2, 1.2, 5.608928571428571\n",
    "1.0, 1.2, 1.3, 5.720039682539682\n",
    "1.0, 1.2, 1.4, 5.720039682539682\n",
    "1.0, 1.3, 0.7, 5.4771825396825395\n",
    "1.0, 1.3, 0.8, 5.4771825396825395\n",
    "1.0, 1.3, 0.9, 5.4771825396825395\n",
    "1.0, 1.3, 1.0, 5.577182539682539\n",
    "1.0, 1.3, 1.1, 5.831150793650793\n",
    "1.0, 1.3, 1.2, 5.831150793650793\n",
    "1.0, 1.3, 1.3, 5.831150793650793\n",
    "1.0, 1.3, 1.4, 5.831150793650793\n",
    "1.0, 1.4, 0.7, 5.653373015873016\n",
    "1.0, 1.4, 0.8, 5.653373015873016\n",
    "1.0, 1.4, 0.9, 5.796230158730159\n",
    "1.0, 1.4, 1.0, 5.796230158730159\n",
    "1.0, 1.4, 1.1, 5.7311507936507935\n",
    "1.0, 1.4, 1.2, 5.7311507936507935\n",
    "1.0, 1.4, 1.3, 5.831150793650793\n",
    "1.0, 1.4, 1.4, 5.331150793650793\n",
    "1.1, 0.7, 0.7, 5.034325396825397\n",
    "1.1, 0.7, 0.8, 5.17718253968254\n",
    "1.1, 0.7, 0.9, 5.034325396825397\n",
    "1.1, 0.7, 1.0, 5.034325396825397\n",
    "1.1, 0.7, 1.1, 5.034325396825397\n",
    "1.1, 0.7, 1.2, 5.045039682539683\n",
    "1.1, 0.7, 1.3, 4.945039682539682\n",
    "1.1, 0.7, 1.4, 5.045039682539683\n",
    "1.1, 0.8, 0.7, 5.134325396825397\n",
    "1.1, 0.8, 0.8, 5.277182539682539\n",
    "1.1, 0.8, 0.9, 5.277182539682539\n",
    "1.1, 0.8, 1.0, 5.277182539682539\n",
    "1.1, 0.8, 1.1, 5.277182539682539\n",
    "1.1, 0.8, 1.2, 5.134325396825397\n",
    "1.1, 0.8, 1.3, 5.134325396825397\n",
    "1.1, 0.8, 1.4, 5.277182539682539\n",
    "1.1, 0.9, 0.7, 5.134325396825397\n",
    "1.1, 0.9, 0.8, 5.27718253968254\n",
    "1.1, 0.9, 0.9, 5.27718253968254\n",
    "1.1, 0.9, 1.0, 5.562896825396826\n",
    "1.1, 0.9, 1.1, 5.277182539682539\n",
    "1.1, 0.9, 1.2, 5.420039682539683\n",
    "1.1, 0.9, 1.3, 5.420039682539683\n",
    "1.1, 0.9, 1.4, 5.420039682539683\n",
    "1.1, 1.0, 0.7, 5.27718253968254\n",
    "1.1, 1.0, 0.8, 5.753373015873016\n",
    "1.1, 1.0, 0.9, 5.753373015873016\n",
    "1.1, 1.0, 1.0, 5.753373015873016\n",
    "1.1, 1.0, 1.1, 5.896230158730159\n",
    "1.1, 1.0, 1.2, 5.896230158730159\n",
    "1.1, 1.0, 1.3, 5.896230158730159\n",
    "1.1, 1.0, 1.4, 5.896230158730159\n",
    "1.1, 1.1, 0.7, 5.134325396825397\n",
    "1.1, 1.1, 0.8, 5.27718253968254\n",
    "1.1, 1.1, 0.9, 5.610515873015873\n",
    "1.1, 1.1, 1.0, 5.610515873015873\n",
    "1.1, 1.1, 1.1, 5.499404761904762\n",
    "1.1, 1.1, 1.2, 5.642261904761905\n",
    "1.1, 1.1, 1.3, 5.642261904761905\n",
    "1.1, 1.1, 1.4, 5.753373015873016\n",
    "1.1, 1.2, 0.7, 5.46765873015873\n",
    "1.1, 1.2, 0.8, 5.46765873015873\n",
    "1.1, 1.2, 0.9, 5.610515873015873\n",
    "1.1, 1.2, 1.0, 5.610515873015873\n",
    "1.1, 1.2, 1.1, 5.753373015873016\n",
    "1.1, 1.2, 1.2, 5.753373015873016\n",
    "1.1, 1.2, 1.3, 5.864484126984127\n",
    "1.1, 1.2, 1.4, 5.831150793650793\n",
    "1.1, 1.3, 0.7, 5.510515873015873\n",
    "1.1, 1.3, 0.8, 5.653373015873016\n",
    "1.1, 1.3, 0.9, 5.653373015873016\n",
    "1.1, 1.3, 1.0, 5.510515873015874\n",
    "1.1, 1.3, 1.1, 5.753373015873016\n",
    "1.1, 1.3, 1.2, 5.864484126984127\n",
    "1.1, 1.3, 1.3, 5.864484126984127\n",
    "1.1, 1.3, 1.4, 5.721626984126984\n",
    "1.1, 1.4, 0.7, 5.4771825396825395\n",
    "1.1, 1.4, 0.8, 5.4771825396825395\n",
    "1.1, 1.4, 0.9, 5.334325396825397\n",
    "1.1, 1.4, 1.0, 5.358134920634921\n",
    "1.1, 1.4, 1.1, 5.612103174603175\n",
    "1.1, 1.4, 1.2, 5.754960317460317\n",
    "1.1, 1.4, 1.3, 5.388293650793651\n",
    "1.1, 1.4, 1.4, 5.388293650793651\n",
    "1.2, 0.7, 0.7, 5.17718253968254\n",
    "1.2, 0.7, 0.8, 5.17718253968254\n",
    "1.2, 0.7, 0.9, 5.17718253968254\n",
    "1.2, 0.7, 1.0, 5.17718253968254\n",
    "1.2, 0.7, 1.1, 5.087896825396825\n",
    "1.2, 0.7, 1.2, 5.087896825396825\n",
    "1.2, 0.7, 1.3, 5.087896825396825\n",
    "1.2, 0.7, 1.4, 5.087896825396825\n",
    "1.2, 0.8, 0.7, 5.277182539682539\n",
    "1.2, 0.8, 0.8, 5.134325396825397\n",
    "1.2, 0.8, 0.9, 5.134325396825397\n",
    "1.2, 0.8, 1.0, 5.134325396825397\n",
    "1.2, 0.8, 1.1, 5.277182539682539\n",
    "1.2, 0.8, 1.2, 5.277182539682539\n",
    "1.2, 0.8, 1.3, 5.187896825396825\n",
    "1.2, 0.8, 1.4, 5.187896825396825\n",
    "1.2, 0.9, 0.7, 5.420039682539683\n",
    "1.2, 0.9, 0.8, 5.420039682539683\n",
    "1.2, 0.9, 0.9, 5.420039682539683\n",
    "1.2, 0.9, 1.0, 5.420039682539683\n",
    "1.2, 0.9, 1.1, 5.27718253968254\n",
    "1.2, 0.9, 1.2, 5.420039682539683\n",
    "1.2, 0.9, 1.3, 5.420039682539683\n",
    "1.2, 0.9, 1.4, 5.308928571428572\n",
    "1.2, 1.0, 0.7, 5.27718253968254\n",
    "1.2, 1.0, 0.8, 5.308928571428572\n",
    "1.2, 1.0, 0.9, 5.642261904761905\n",
    "1.2, 1.0, 1.0, 5.451785714285714\n",
    "1.2, 1.0, 1.1, 5.166071428571429\n",
    "1.2, 1.0, 1.2, 5.308928571428572\n",
    "1.2, 1.0, 1.3, 5.451785714285714\n",
    "1.2, 1.0, 1.4, 5.785119047619047\n",
    "1.2, 1.1, 0.7, 5.46765873015873\n",
    "1.2, 1.1, 0.8, 5.610515873015873\n",
    "1.2, 1.1, 0.9, 5.610515873015873\n",
    "1.2, 1.1, 1.0, 5.753373015873016\n",
    "1.2, 1.1, 1.1, 5.610515873015873\n",
    "1.2, 1.1, 1.2, 5.610515873015873\n",
    "1.2, 1.1, 1.3, 5.753373015873016\n",
    "1.2, 1.1, 1.4, 5.753373015873016\n",
    "1.2, 1.2, 0.7, 5.510515873015873\n",
    "1.2, 1.2, 0.8, 5.653373015873016\n",
    "1.2, 1.2, 0.9, 5.653373015873016\n",
    "1.2, 1.2, 1.0, 5.753373015873016\n",
    "1.2, 1.2, 1.1, 5.753373015873016\n",
    "1.2, 1.2, 1.2, 5.753373015873016\n",
    "1.2, 1.2, 1.3, 5.562896825396825\n",
    "1.2, 1.2, 1.4, 5.674007936507936\n",
    "1.2, 1.3, 0.7, 5.510515873015874\n",
    "1.2, 1.3, 0.8, 5.510515873015874\n",
    "1.2, 1.3, 0.9, 5.510515873015874\n",
    "1.2, 1.3, 1.0, 5.653373015873016\n",
    "1.2, 1.3, 1.1, 5.753373015873016\n",
    "1.2, 1.3, 1.2, 5.896230158730159\n",
    "1.2, 1.3, 1.3, 5.896230158730159\n",
    "1.2, 1.3, 1.4, 6.00734126984127\n",
    "1.2, 1.4, 0.7, 5.215277777777778\n",
    "1.2, 1.4, 0.8, 5.215277777777778\n",
    "1.2, 1.4, 0.9, 5.35813492063492\n",
    "1.2, 1.4, 1.0, 5.500992063492063\n",
    "1.2, 1.4, 1.1, 5.500992063492063\n",
    "1.2, 1.4, 1.2, 5.712103174603174\n",
    "1.2, 1.4, 1.3, 5.212103174603174\n",
    "1.2, 1.4, 1.4, 5.212103174603174\n",
    "1.3, 0.7, 0.7, 5.17718253968254\n",
    "1.3, 0.7, 0.8, 5.17718253968254\n",
    "1.3, 0.7, 0.9, 5.17718253968254\n",
    "1.3, 0.7, 1.0, 5.17718253968254\n",
    "1.3, 0.7, 1.1, 5.320039682539682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e11555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
